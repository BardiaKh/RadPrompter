{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Basic Use Cases\n",
    "\n",
    "In this tutorial, we are going to see how we can use **RadPrompter** for simplified and reproducible LLM prompting. We will cover more advanced capabilities in the next tutorials.\n",
    "\n",
    "RadPrompter uses three simple components:\n",
    "\n",
    "1. **Prompt**: The `Prompt` is the core message-passing recipe between the user and the model.\n",
    "2. **Client**: The `Client` is responsible for contacting various LLM servers.\n",
    "3. **Engine**: The `Engine` is the core functionality that ties the other two components together and coordinates running a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "If you don't have `RadPrompter` installed, you can install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install radprompter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Let's start with the `Prompt`. We first have to import it from the **RadPrompter** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts can be initiated by a toml file. Let's read `01_Basic-Usecase.toml` to see its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METADATA]\n",
      "\n",
      "version = 0.1\n",
      "description = \"A sample prompt for RadPrompter\"\n",
      "\n",
      "\n",
      "[CONSTRUCTOR]\n",
      "system = \"You are an experienced radiologist that help users extract infromation from radiology reports.\"\n",
      "user = \"\"\"Does the following report indicate a normal or abnormal finding?\n",
      "{{report}}\n",
      "\n",
      "Just reply with \"normal\" or \"abnormal\" to indicate your answer, without any additional information.\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "with open(\"./01_Basic-Usecase.toml\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "print(\"\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the minimum requirements for a prompt file are to have a `[METADATA]` and a `[CONSTRUCTOR]` section. The metadata is used for version tracking and verbose communication. The constructor is used to craft the LLM chat structure.\n",
    "\n",
    "In this example, we specify a `system` attribute which will be the system prompt for the LLM. We also have a `user` attribute which contains the user's request. You might notice the `{{report}}` tag being enclosed in double curly brackets. This denotes a **placeholder** item that can be replaced with your actual report. Note that this curly bracket placeholder notation can be mixed with any arbitrary variable name that you like, and \"report\" is just an example. We will learn more about this as we move forward.\n",
    "\n",
    "Let's now create our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='padding: 0; border-radius: 5px; font-family: Arial; line-height: 1.2rem; border: 1px solid currentColor'><div style='display: flex; align-items: top; padding: 0; border-right-width: 1px'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px; '>System:</h4><p style='margin: 0; padding: 8px; border-left: 1px solid currentColor;'>You are an experienced radiologist that help users extract infromation from radiology reports.</p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>User:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'>Does the following report indicate a normal or abnormal finding?<br><span style='background-color: rgb(255, 224, 178, 0.3);'>{{report}}</span><br><br>Just reply with &quot;normal&quot; or &quot;abnormal&quot; to indicate your answer, without any additional information.<br></p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>Assistant:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'><span style='background-color: rgb(178, 219, 255, 0.3);'>[... response ...]</span></p></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = Prompt(\"./01_Basic-Usecase.toml\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each prompt comes with a tabular visualization for easier interpretation. The placeholders will be in a light orange color, and the assistant's response will replace the blue `[... response ...]` tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client\n",
    "\n",
    "RadPrompter supports several LLM clients out of the box, including:\n",
    "\n",
    "- `OpenAIClient`: For accessing OpenAI's models\n",
    "- `vLLMClient`: For accessing open-source LLMs like Llama hosted using the [vLLM package](https://vllm.ai/).\n",
    "- `OllamaClient`: For accesing [Ollama](https://ollama.com/) open source models.\n",
    "- `HuggingFaceClient`: For accesing [HuggingFace](https://huggingface.co/) open source models.\n",
    "\n",
    "To instantiate a client, you need to provide the following:\n",
    "\n",
    "1. `model` [Required]: The name of the model to use (e.g., \"gpt-3.5-turbo\" for OpenAI,  \"meta-llama/Meta-Llama-3-8B-Instruct\" for vLLM and \"phi3\" for Ollama).\n",
    "2. `base_url` [Optional]: The URL of the REST API endpoint.\n",
    "3. `api_key` [Optional]: Your API key for the service. If not provided, the client will attempt to read it from an environment variable (in case of `OpenAIClient`) or set that to \"EMPTY\" (`vLLMClient` and `OllamaClient`).\n",
    "4. `temperature` [Optional]: Sampling temperature for the LLM (Default: `0.0`).\n",
    "5. `seed` [Optional]: Sampling seed for the LLM (Default: `42`).\n",
    "\n",
    "Here's an example of instantiating a vLLMClient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import vLLMClient\n",
    "\n",
    "client = vLLMClient(\n",
    "    model = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    base_url = \"http://localhost:9999/v1\",\n",
    "    temperature = 0.0,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up our client, we need to connect everything together using the RadPrompter **Engine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine\n",
    "\n",
    "The `RadPrompter` class is the engine that ties everything together. To use it, we need to provide:\n",
    "\n",
    "1. `client`: The LLM client to use \n",
    "2. `prompt`: The prompt object\n",
    "3. `output_file`: The path to save the results to (must be a .csv file)\n",
    "4. `concurrency`: As we are hitting a server with requests, you can batch your requests together for faster processing (Default: `1`). \n",
    "\n",
    "Let's instantiate the engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import RadPrompter\n",
    "\n",
    "engine = RadPrompter(\n",
    "    client=client,\n",
    "    prompt=prompt, \n",
    "    output_file=\"output_tutorial_1.csv\",\n",
    "    concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the engine, we pass it a list of dictionaries. Each dictionary should contain keys matching the placeholders in the prompt. Any additional keys will be included in the output file. \n",
    "\n",
    "So let's create a list of our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'report': 'Clinical Information:\\n67-year-old male with shortness of breath and pleuritic chest pain. Rule out pulmonary embolism.\\n\\nTechnique:\\nCT pulmonary angiography with IV contrast was performed.\\n\\nFindings:\\nThere are filling defects within the right and left main pulmonary arteries extending into the lobar branches, compatible with bilateral pulmonary emboli. No evidence of right heart strain.\\n\\nGround glass opacities are present within the peripheral lungs bilaterally, suggesting a component of hemorrhage.\\n\\nThe main pulmonary arteries are dilated, with a diameter of 3.4 cm on the right and 3.1 cm on the left (upper limits of normal).\\n\\nNo pleural effusion is identified.\\n\\nImpression:\\n1. Bilateral pulmonary emboli involving the right and left main pulmonary arteries and lobar branches.\\n2. Findings suggestive of pulmonary hemorrhage.\\n3. Pulmonary artery dilation suggesting chronic pulmonary hypertension.\\n\\nRecommendation:\\nClinical correlation is recommended. Anticoagulation therapy should be initiated if not already started. Consider echocardiography to evaluate for right heart strain.',\n",
       " 'file_name': '../../sample_reports/sample_report_1.txt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "report_files = glob.glob(\"../../sample_reports/*.txt\")\n",
    "\n",
    "reports = []\n",
    "for file in report_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        reports.append({\"report\": f.read(), \"file_name\": file})\n",
    "        \n",
    "reports[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of 3 reports, each having a `report` key (which on every run will replace the `{{report}}` placeholder in our prompt) and a `file_name` key that will be included in the output csv file for report identification.\n",
    "\n",
    "Now let's run our `engine`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 3/3 [00:00<00:00, 15.28it/s]\n"
     ]
    }
   ],
   "source": [
    "engine(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The engine will process each report and save the results to `output_tutorial_1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_response</th>\n",
       "      <th>report</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abnormal</td>\n",
       "      <td>Clinical Information:\\n72-year-old female with...</td>\n",
       "      <td>../../sample_reports/sample_report_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abnormal</td>\n",
       "      <td>Clinical Information:\\n67-year-old male with s...</td>\n",
       "      <td>../../sample_reports/sample_report_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abnormal</td>\n",
       "      <td>Here is an example radiology report describing...</td>\n",
       "      <td>../../sample_reports/sample_report_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      default_response                                             report  \\\n",
       "index                                                                       \n",
       "1             Abnormal  Clinical Information:\\n72-year-old female with...   \n",
       "0             Abnormal  Clinical Information:\\n67-year-old male with s...   \n",
       "2             Abnormal  Here is an example radiology report describing...   \n",
       "\n",
       "                                      file_name  \n",
       "index                                            \n",
       "1      ../../sample_reports/sample_report_2.txt  \n",
       "0      ../../sample_reports/sample_report_1.txt  \n",
       "2      ../../sample_reports/sample_report_3.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output_tutorial_1.csv\", index_col='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `default_response` will contain the LLM output. In future tutorials, you will learn how to use **Schemas** to further customize your output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save a log of the run using the `save_log` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RadPrompter Version: 1.1.3\n",
      "Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Prompt TOML: /mnt/NAS3/homes/bkhosra/RadPrompter/tutorials/01_Basic-Usecase/01_Basic-Usecase.toml\n",
      "Prompt Version: 0.1\n",
      "Prompt Hash: ce8b273997755ad7279e9c7a13337c70\n",
      "Concurrency Factor: 2\n",
      "Start Time: 2024-05-20 09:35:59\n",
      "End Time: 2024-05-20 09:35:59\n",
      "Duration: 0.0\n",
      "Number of Items: 3\n",
      "Average Processing Time: 0.0\n",
      "\n",
      "\n",
      "-------------------- *** - Prompt Content - *** --------------------\n",
      "[METADATA]\n",
      "\n",
      "version = 0.1\n",
      "description = \"A sample prompt for RadPrompter\"\n",
      "\n",
      "\n",
      "[CONSTRUCTOR]\n",
      "system = \"You are an experienced radiologist that help users extract infromation from radiology reports.\"\n",
      "user = \"\"\"Does the following report indicate a normal or abnormal finding?\n",
      "{{report}}\n",
      "\n",
      "Just reply with \"normal\" or \"abnormal\" to indicate your answer, without any additional information.\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "engine.save_log(\"log_tutorial_1.log\")\n",
    "\n",
    "with open(\"log_tutorial_1.log\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save detailed information about the run, including the prompt used, to the specified file.\n",
    "\n",
    "And that's it for the basics of using RadPrompter! In the next tutorials we'll cover more advanced features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RadPrompter",
   "language": "python",
   "name": "rp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
