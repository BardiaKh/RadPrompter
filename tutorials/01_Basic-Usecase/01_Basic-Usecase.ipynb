{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Basic Use Cases\n",
    "\n",
    "In this tutorial, we are going to see how we can use **RadPrompter** for simplified and reproducible LLM prompting. We will cover more advanced capabilities in the next tutorials.\n",
    "\n",
    "RadPrompter uses three simple components:\n",
    "\n",
    "1. **Prompt**: The `Prompt` is the core message-passing recipe between the user and the model.\n",
    "2. **Client**: The `Client` is responsible for contacting various LLM servers.\n",
    "3. **Engine**: The `Engine` is the core functionality that ties the other two components together and coordinates running a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Let's start with the `Prompt`. We first have to import it from the **RadPrompter** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts can be initiated by a toml file. Let's read `01_Basic-Usecase.toml` to see its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METADATA]\n",
      "\n",
      "version = 0.1\n",
      "description = \"A sample prompt for RadPrompter\"\n",
      "\n",
      "\n",
      "[CONSTRUCTOR]\n",
      "system = \"You are an experienced radiologist that help users extract infromation from radiology reports.\"\n",
      "user = \"\"\"\"Does the following report indicate a normal or abnormal finding?\n",
      "{{report}}\n",
      "\n",
      "Just reply with \"normal\" or \"abnormal\" to indicate your answer, without any additional information.\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "with open(\"./01_Basic-Usecase.toml\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "print(\"\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the minimum requirements for a prompt file are to have a `[METADATA]` and a `[CONSTRUCTOR]` section. The metadata is used for version tracking and verbose communication. The constructor is used to craft the LLM chat structure.\n",
    "\n",
    "In this example, we specify a `system` attribute which will be the system prompt for the LLM. We also have a `user` attribute which contains the user's request. You might notice the `{{report}}` tag being enclosed in double curly brackets. This denotes a **placeholder** item that can be replaced with your actual report. Note that this curly bracket placeholder notation can be mixed with any arbitrary variable name that you like, and \"report\" is just an example. We will learn more about this as we move forward.\n",
    "\n",
    "Let's now create our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='padding: 0; border-radius: 5px; font-family: Arial; line-height: 1.2rem; border: 1px solid currentColor'><div style='display: flex; align-items: top; padding: 0; border-right-width: 1px'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px; '>System:</h4><p style='margin: 0; padding: 8px; border-left: 1px solid currentColor;'>You are an experienced radiologist that help users extract infromation from radiology reports.</p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>User:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'>&quot;Does the following report indicate a normal or abnormal finding?<br><span style='background-color: rgb(255, 224, 178, 0.3);'>{{report}}</span><br><br>Just reply with &quot;normal&quot; or &quot;abnormal&quot; to indicate your answer, without any additional information.<br></p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>Assistant:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'><span style='background-color: rgb(178, 219, 255, 0.3);'>[... response ...]</span></p></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = Prompt(\"./01_Basic-Usecase.toml\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each prompt comes with a tabular visualization for easier interpretation. The placeholders will be in a light orange color, and the assistant's response will replace the blue `[... response ...]` tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client\n",
    "\n",
    "RadPrompter supports several LLM clients out of the box, including:\n",
    "\n",
    "- `OpenAIClient`: For accessing OpenAI's models\n",
    "- `vLLMClient`: For accessing open-source LLMs like Llama hosted using the [vLLM package](https://vllm.ai/).\n",
    "- `OllamaClient`: For accesing [Ollama](https://ollama.com/) open source models.\n",
    "\n",
    "To instantiate a client, you need to provide the following:\n",
    "\n",
    "1. `model` [Required]: The name of the model to use (e.g., \"gpt-3.5-turbo\" for OpenAI,  \"meta-llama/Meta-Llama-3-8B-Instruct\" for vLLM and \"phi3\" for Ollama).\n",
    "2. `base_url` [Optional]: The URL of the REST API endpoint.\n",
    "3. `api_key` [Optional]: Your API key for the service. If not provided, the client will attempt to read it from an environment variable (in case of `OpenAIClient`) or set that to \"EMPTY\" (`vLLMClient` and `OllamaClient`).\n",
    "4. `temperature` [Optional]: Sampling temperature for the LLM (Default: `0.0`).\n",
    "5. `seed` [Optional]: Sampling seed for the LLM (Default: `42`).\n",
    "\n",
    "Here's an example of instantiating a vLLMClient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import vLLMClient\n",
    "\n",
    "client = vLLMClient(\n",
    "    model = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    base_url = \"http://localhost:9999/v1\",\n",
    "    temperature = 0.0,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up our client, we need to coonect everything together using the RadPrompter **Engine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine\n",
    "\n",
    "The `RadPrompter` class is the engine that ties everything together. To use it, we need to provide:\n",
    "\n",
    "1. `client`: The LLM client to use \n",
    "2. `prompt`: The prompt object\n",
    "3. `output_file`: The path to save the results to (must be a .csv file)\n",
    "4. `concurrency`: As we are hitting a server with requests, you can batch your requests together for faster processing (Default: `1`). \n",
    "\n",
    "Let's instantiate the engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import RadPrompter\n",
    "\n",
    "engine = RadPrompter(\n",
    "    client=client,\n",
    "    prompt=prompt, \n",
    "    output_file=\"output_tutorial_1.csv\",\n",
    "    concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the engine, we pass it a list of dictionaries. Each dictionary should contain keys matching the placeholders in the prompt. Any additional keys will be included in the output file. \n",
    "\n",
    "So let's create a list of our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'report': 'Clinical Information:\\n72-year-old female with sudden onset shortness of breath and hypoxia. Evaluate for pulmonary embolism.\\nTechnique:\\nCT pulmonary angiography with IV contrast was performed.\\nFindings:\\nThere is a large saddle embolus extending from the main pulmonary artery into the right and left main pulmonary arteries. Multiple filling defects are also seen within the segmental and subsegmental branches bilaterally, indicating additional pulmonary emboli.\\nThe right ventricle appears dilated at 4.6 cm, greater than the left ventricle. There is flattening of the interventricular septum, suggesting right heart strain.\\nMosaic perfusion is present within the lungs bilaterally, likely representing pulmonary infarcts.\\nSmall bilateral pleural effusions are noted.\\nImpression:\\n\\nSaddle pulmonary embolism involving the main pulmonary artery, right and left main branches, and multiple segmental/subsegmental branches.\\nFindings of right heart strain with right ventricular dilation and interventricular septal flattening.\\nBilateral pleural effusions, likely due to pulmonary infarcts.\\n\\nRecommendation:\\nEmergent anticoagulation therapy is recommended given the large burden of pulmonary arterial clot and evidence of right heart strain. Surgical or catheter-based embolectomy may need to be considered. Echocardiography can further evaluate right heart function and pulmonary pressures.',\n",
       " 'file_name': '../../sample_reports/sample_report_2.txt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "report_files = glob.glob(\"../../sample_reports/*.txt\")\n",
    "\n",
    "reports = []\n",
    "for file in report_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        reports.append({\"report\": f.read(), \"file_name\": file})\n",
    "        \n",
    "reports[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of 3 reports, each having a `report` key (which on every run will replace the `{{report}}` placeholder in our prompt) and a `file_name` key that will be included in the output csv file for report identification.\n",
    "\n",
    "Now let's run our `engine`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 100%|██████████| 3/3 [00:00<00:00, 11.13it/s]\n"
     ]
    }
   ],
   "source": [
    "engine(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The engine will process each report and save the results to `output_tutorial_1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_response</th>\n",
       "      <th>report</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abnormal</td>\n",
       "      <td>Clinical Information:\\n72-year-old female with...</td>\n",
       "      <td>../../sample_reports/sample_report_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abnormal</td>\n",
       "      <td>Here is an example radiology report describing...</td>\n",
       "      <td>../../sample_reports/sample_report_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abnormal</td>\n",
       "      <td>Clinical Information:\\n67-year-old male with s...</td>\n",
       "      <td>../../sample_reports/sample_report_1.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      default_response                                             report  \\\n",
       "index                                                                       \n",
       "0             Abnormal  Clinical Information:\\n72-year-old female with...   \n",
       "1             Abnormal  Here is an example radiology report describing...   \n",
       "2             Abnormal  Clinical Information:\\n67-year-old male with s...   \n",
       "\n",
       "                                      file_name  \n",
       "index                                            \n",
       "0      ../../sample_reports/sample_report_2.txt  \n",
       "1      ../../sample_reports/sample_report_3.txt  \n",
       "2      ../../sample_reports/sample_report_1.txt  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output_tutorial_1.csv\", index_col='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `default_response` will contain the LLM output. In future tutorials, you will learn how to use **Schemas** to further customize your output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save a log of the run using the `save_log` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RadPrompter Version: 1.0.8\n",
      "Model: meta-llama/Meta-Llama-3-8B-Instruct\n",
      "Prompt TOML: /Users/bardiakhosravi/Desktop/GitHub/RadPrompter/tutorials/01_Basic-Usecase/01_Basic-Usecase.toml\n",
      "Prompt Version: 0.1\n",
      "Prompt Hash: 369eb088846feaa3e6ea0fdc3a1a40b2\n",
      "Concurrency Factor: 2\n",
      "Start Time: 2024-05-19 14:14:28\n",
      "End Time: 2024-05-19 14:14:28\n",
      "Duration: 0.0\n",
      "Number of Items: 3\n",
      "Average Processing Time: 0.0\n",
      "\n",
      "\n",
      "-------------------- *** - Prompt Content - *** --------------------\n",
      "[METADATA]\n",
      "\n",
      "version = 0.1\n",
      "description = \"A sample prompt for RadPrompter\"\n",
      "\n",
      "\n",
      "[CONSTRUCTOR]\n",
      "system = \"You are an experienced radiologist that help users extract infromation from radiology reports.\"\n",
      "user = \"\"\"\"Does the following report indicate a normal or abnormal finding?\n",
      "{{report}}\n",
      "\n",
      "Just reply with \"normal\" or \"abnormal\" to indicate your answer, without any additional information.\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "engine.save_log(\"log_tutorial_1.log\")\n",
    "\n",
    "with open(\"log_tutorial_1.log\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save detailed information about the run, including the prompt used, to the specified file.\n",
    "\n",
    "And that's it for the basics of using RadPrompter! In the next tutorials we'll cover more advanced features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
