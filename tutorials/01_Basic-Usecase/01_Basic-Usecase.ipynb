{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Basic Use Cases\n",
    "\n",
    "In this tutorial, we are going to see how we can use **RadPrompter** for simplified and reproducible LLM prompting. We will cover more advanced capabilities in the next tutorials.\n",
    "\n",
    "RadPrompter uses three simple components:\n",
    "\n",
    "1. **Prompt**: The `Prompt` is the core message-passing recipe between the user and the model.\n",
    "2. **Client**: The `Client` is responsible for contacting various LLM servers.\n",
    "3. **Engine**: The `Engine` is the core functionality that ties the other two components together and coordinates running a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Let's start with the `Prompt`. We first have to import it from the **RadPrompter** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts can be initiated by a toml file. Let's read `01_Basic-Usecase.toml` to see its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./01_Basic-Usecase.toml\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "print(\"\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the minimum requirements for a prompt file are to have a `[METADATA]` and a `[CONSTRUCTOR]` section. The metadata is used for version tracking and verbose communication. The constructor is used to craft the LLM chat structure.\n",
    "\n",
    "In this example, we specify a `system` attribute which will be the system prompt for the LLM. We also have a `user` attribute which contains the user's request. You might notice the `{{report}}` tag being enclosed in double curly brackets. This denotes a **placeholder** item that can be replaced with your actual report. Note that this curly bracket placeholder notation can be mixed with any arbitrary variable name that you like, and \"report\" is just an example. We will learn more about this as we move forward.\n",
    "\n",
    "Let's now create our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = Prompt(\"./01_Basic-Usecase.toml\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each prompt comes with a tabular visualization for easier interpretation. The placeholders will be in a light orange color, and the assistant's response will replace the blue `[... response ...]` tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client\n",
    "\n",
    "RadPrompter supports several LLM clients out of the box, including:\n",
    "\n",
    "- `OpenAIClient`: For accessing OpenAI's models\n",
    "- `vLLMClient`: For accessing open-source LLMs like Llama hosted using the [vLLM package](https://vllm.ai/).\n",
    "- `OllamaClient`: For accesing [Ollama](https://ollama.com/) open source models.\n",
    "- `HuggingFaceClient`: For accesing [HuggingFace](https://huggingface.co/) open source models.\n",
    "\n",
    "To instantiate a client, you need to provide the following:\n",
    "\n",
    "1. `model` [Required]: The name of the model to use (e.g., \"gpt-3.5-turbo\" for OpenAI,  \"meta-llama/Meta-Llama-3-8B-Instruct\" for vLLM and \"phi3\" for Ollama).\n",
    "2. `base_url` [Optional]: The URL of the REST API endpoint.\n",
    "3. `api_key` [Optional]: Your API key for the service. If not provided, the client will attempt to read it from an environment variable (in case of `OpenAIClient`) or set that to \"EMPTY\" (`vLLMClient` and `OllamaClient`).\n",
    "4. `temperature` [Optional]: Sampling temperature for the LLM (Default: `0.0`).\n",
    "5. `seed` [Optional]: Sampling seed for the LLM (Default: `42`).\n",
    "\n",
    "Here's an example of instantiating a vLLMClient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import vLLMClient\n",
    "\n",
    "client = vLLMClient(\n",
    "    model = \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    base_url = \"http://localhost:9999/v1\",\n",
    "    temperature = 0.0,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up our client, we need to coonect everything together using the RadPrompter **Engine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine\n",
    "\n",
    "The `RadPrompter` class is the engine that ties everything together. To use it, we need to provide:\n",
    "\n",
    "1. `client`: The LLM client to use \n",
    "2. `prompt`: The prompt object\n",
    "3. `output_file`: The path to save the results to (must be a .csv file)\n",
    "4. `concurrency`: As we are hitting a server with requests, you can batch your requests together for faster processing (Default: `1`). \n",
    "\n",
    "Let's instantiate the engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import RadPrompter\n",
    "\n",
    "engine = RadPrompter(\n",
    "    client=client,\n",
    "    prompt=prompt, \n",
    "    output_file=\"output_tutorial_1.csv\",\n",
    "    concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the engine, we pass it a list of dictionaries. Each dictionary should contain keys matching the placeholders in the prompt. Any additional keys will be included in the output file. \n",
    "\n",
    "So let's create a list of our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "report_files = glob.glob(\"../../sample_reports/*.txt\")\n",
    "\n",
    "reports = []\n",
    "for file in report_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        reports.append({\"report\": f.read(), \"file_name\": file})\n",
    "        \n",
    "reports[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of 3 reports, each having a `report` key (which on every run will replace the `{{report}}` placeholder in our prompt) and a `file_name` key that will be included in the output csv file for report identification.\n",
    "\n",
    "Now let's run our `engine`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The engine will process each report and save the results to `output_tutorial_1.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output_tutorial_1.csv\", index_col='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `default_response` will contain the LLM output. In future tutorials, you will learn how to use **Schemas** to further customize your output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also save a log of the run using the `save_log` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save_log(\"log_tutorial_1.log\")\n",
    "\n",
    "with open(\"log_tutorial_1.log\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will save detailed information about the run, including the prompt used, to the specified file.\n",
    "\n",
    "And that's it for the basics of using RadPrompter! In the next tutorials we'll cover more advanced features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RadPrompter",
   "language": "python",
   "name": "rp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
