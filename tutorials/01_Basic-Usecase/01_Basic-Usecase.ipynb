{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: Basic Use Cases\n",
    "\n",
    "In this tutorial, we are going to see how we can use **RadPrompter** for simplified and reproducible LLM prompting. We will cover more advanced capabilities in the next tutorials.\n",
    "\n",
    "RadPrompter uses three simple components:\n",
    "\n",
    "1. **Prompt**: The `Prompt` is the core message-passing recipe between the user and the model.\n",
    "2. **Client**: The `Client` is responsible for contacting various LLM servers.\n",
    "3. **Engine**: The `Engine` is the core functionality that ties the other two components together and coordinates running a job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Let's start with the `Prompt`. We first have to import it from the **RadPrompter** package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts can be initiated by a toml file. Let's read `01_Basic-Usecase.toml` to see its structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[METADATA]\n",
      "\n",
      "version = 0.1\n",
      "description = \"A sample prompt for RadPrompter\"\n",
      "\n",
      "\n",
      "[CONSTRUCTOR]\n",
      "system = \"You are an experienced radiologist that help users extract infromation from radiology reports.\"\n",
      "user = \"\"\"\"Does the following report indicate a normal or abnormal finding?\n",
      "{{report}}\n",
      "\n",
      "Just reply with \"normal\" or \"abnormal\" to indicate your answer, without any additional information.\n",
      "\"\"\"\n"
     ]
    }
   ],
   "source": [
    "with open(\"./01_Basic-Usecase.toml\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "print(\"\".join(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the minimum requirements for a prompt file are to have a `[METADATA]` and a `[CONSTRUCTOR]` section. The metadata is used for version tracking and verbose communication. The constructor is used to craft the LLM chat structure.\n",
    "\n",
    "In this example, we specify a `system` attribute which will be the system prompt for the LLM. We also have a `user` attribute which contains the user's request. You might notice the `{{report}}` tag being enclosed in double curly brackets. This denotes a **placeholder** item that can be replaced with your actual report. Note that this curly bracket placeholder notation can be mixed with any arbitrary variable name that you like, and \"report\" is just an example. We will learn more about this as we move forward.\n",
    "\n",
    "Let's now create our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='padding: 0; border-radius: 5px; font-family: Arial; line-height: 1.2rem; border: 1px solid currentColor'><div style='display: flex; align-items: top; padding: 0; border-right-width: 1px'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px; '>System:</h4><p style='margin: 0; padding: 8px; border-left: 1px solid currentColor;'>You are an experienced radiologist that help users extract infromation from radiology reports.</p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>User:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'>&quot;Does the following report indicate a normal or abnormal finding?<br><span style='background-color: rgb(255, 224, 178, 0.3);'>{{report}}</span><br><br>Just reply with &quot;normal&quot; or &quot;abnormal&quot; to indicate your answer, without any additional information.<br></p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>Assistant:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'><span style='background-color: rgb(178, 219, 255, 0.3);'>[... response ...]</span></p></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = Prompt(\"./01_Basic-Usecase.toml\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, each prompt comes with a tabular visualization for easier interpretation. The placeholders will be in a light orange color, and the assistant's response will replace the blue `[... response ...]` tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client\n",
    "\n",
    "RadPrompter supports several LLM clients out of the box, including:\n",
    "\n",
    "- `OpenAIClient`: For accessing OpenAI's models\n",
    "- `vLLMClient`: For accessing open-source LLMs like Llama hosted using the [vLLM package](https://vllm.ai/).\n",
    "- `OllamaClient`: For accesing [Ollama](https://ollama.com/) open source models.\n",
    "\n",
    "To instantiate a client, you need to provide the following:\n",
    "\n",
    "1. `model` [Required]: The name of the model to use (e.g., \"gpt-3.5-turbo\" for OpenAI,  \"meta-llama/Meta-Llama-3-8B-Instruct\" for vLLM and \"phi3\" for Ollama).\n",
    "2. `base_url` [Optional]: The URL of the REST API endpoint.\n",
    "3. `api_key` [Optional]): Your API key for the service. If not provided, the client will attempt to read it from an environment variable (in case of `OpenAIClient`) or set that to \"EMPTY\" (`vLLMClient` and `OllamaClient`).\n",
    "\n",
    "Here's an example of instantiating a vLLMClient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import vLLMClient\n",
    "\n",
    "client = vLLMClient(\n",
    "    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    base_url=\"http://localhost:9999\",\n",
    "    api_key=\"EMPTY\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have set up our client, we need to coonect everything together using the RadPrompter **Engine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engine\n",
    "\n",
    "The `RadPrompter` class is the engine that ties everything together. To use it, we need to provide:\n",
    "\n",
    "1. `client`: The LLM client to use \n",
    "2. `prompt`: The prompt object\n",
    "3. `output_file`: The path to save the results to (must be a .csv file)\n",
    "\n",
    "Let's instantiate the engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import RadPrompter\n",
    "\n",
    "engine = RadPrompter(\n",
    "    client=client,\n",
    "    prompt=prompt, \n",
    "    output_file=\"output_tutorial_1.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the engine, we pass it a list of dictionaries. Each dictionary should contain keys matching the placeholders in the prompt. Any additional keys will be included in the output file. \n",
    "\n",
    "So let's create a list of our inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "report_files = glob.glob(\"../../sample_reports/*.txt\")\n",
    "\n",
    "reports = []\n",
    "for file in report_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        reports.append({\"report\": f.read(), \"file_name\": file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mengine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreports\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Dev/envs/CV/lib/python3.11/site-packages/radprompter/radprompter.py:111\u001b[0m, in \u001b[0;36mRadPrompter.__call__\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (datetime\u001b[38;5;241m.\u001b[39mstrptime(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEnd Time\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m \n\u001b[1;32m    109\u001b[0m                         datetime\u001b[38;5;241m.\u001b[39mstrptime(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStart Time\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m.\u001b[39mtotal_seconds()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of Items\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(items)\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage Processing Time\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDuration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNumber of Items\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "engine(reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
