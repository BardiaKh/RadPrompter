{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89716955-cfb6-4d70-bb82-0c6cc864e907",
   "metadata": {},
   "source": [
    "# Tutorial 6: HuggingFace Client\n",
    "\n",
    "In this tutorial, we'll explore how to use the `HuggingFaceClient`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f9c2d",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "If you don't have `RadPrompter` installed, you can install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install radprompter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7ff19-6d51-4b73-a0c2-c154090503aa",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "As always, we start by importing the `Prompt` class and creating a prompt object from a TOML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8a55b9-77be-4443-8568-37159027379c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='padding: 0; border-radius: 5px; font-family: Arial; line-height: 1.2rem; border: 1px solid currentColor'><div style='display: flex; align-items: top; padding: 0; border-right-width: 1px'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px; '>System:</h4><p style='margin: 0; padding: 8px; border-left: 1px solid currentColor;'>You are a helpful assistant that has 20 years of experience in reading radiology reports and extracting data elements.</p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>User:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'>Carefully review the provided chest CT report (in the &lt;report&gt; tag). Ensure that each data element is accurately captured. Here is the report:<br>&lt;report&gt;<br><span style='background-color: rgb(255, 224, 178, 0.3);'>{{report}}</span><br>&lt;/report&gt;<br>I want you to extract the following data element from the report: &#x27;Pulmonary Embolism&#x27;<br>Here are your options and you can explicitly use one of these:<br>  - `Present`<br>  - `Absent`<br>Hint: &quot;Indicate `Present` if the report explicitly mentions the patient has pulmonary embolism in their CT scan. Indicate `Absent` if pulmonary embolism is not seen or if a previously observed pulmonary embolism is mentioned as resolved.<br>After you provide the data element, I will ask you to provide an explanation and then the final answer.<br>Now give your initial answer. Then provide a step-by-step explanation based on the information in the report, using no more than three short sentences. You can use less sentences if needed.Try to critically appraise your initial answer, which MIGHT be wrong. Then give me your final answer.<br>Format your answers with this format as:<br>&lt;answer&gt;<br>&lt;initial_answer&gt;<br>initial answer goes here<br>&lt;/initial_answer&gt;<br>&lt;explanation&gt;<br>1. your first explanation goes here<br>2. your second explanation goes here (if needed)<br>3. your third explanation goes here (if needed) <br>&lt;/explanation&gt;<br>&lt;final_answer&gt;<br>final answer goes here<br>&lt;/final_answer&gt;<br>&lt;/answer&gt;<br></p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>Assistant:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'>&lt;answer&gt;<span style='background-color: rgb(178, 219, 255, 0.3);'>[... response ...]</span>&lt;/answer&gt;</p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>User:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'>Great. Now please format your response in JSON format like this:<br>{<br>    &quot;answer&quot;: &quot;your final answer&quot;<br>}<br></p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>Assistant:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'><span style='background-color: rgb(178, 219, 255, 0.3);'>[... response ...]</span>}</p></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from radprompter import Prompt\n",
    "\n",
    "prompt = Prompt('06_HuggingFace-Client.toml')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e829b2c-79b3-45c4-8b40-1b3744fd6c29",
   "metadata": {},
   "source": [
    "## Client and Engine\n",
    "\n",
    "We'll use the new `HuggingFaceClient`. This model accepts the model and tokenizer from HuggingFace. This allows for all customizations on the model including quantization.\n",
    "\n",
    "Before running the next cell, make sure you have the following libraries installed:\n",
    "\n",
    "```bash\n",
    "pip install torch transformers flash_attn SentencePiece accelerate bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7730bec-691b-4727-98b8-f0ea4868a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971d0583f3de4c169c8e30b65c20a68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "quantization_conf = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_conf, device_map=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be337f7-fbf2-40e6-b592-47c883402ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from radprompter import RadPrompter, HuggingFaceClient\n",
    "\n",
    "client = HuggingFaceClient(\n",
    "    hf_model = model,\n",
    "    hf_tokenizer = tokenizer,\n",
    "    temperature = 0.0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "engine = RadPrompter(\n",
    "    client=client,\n",
    "    prompt=prompt, \n",
    "    output_file=\"output_tutorial_6.csv\",\n",
    "    concurrency=1,\n",
    "    hide_blocks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc605fa7-99a5-47fa-a3b9-d4e3c23dbe54",
   "metadata": {},
   "source": [
    "And we run it on our sample reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00be2ab6-e689-4dac-ab48-60c1a6d72167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:   0%|          | 0/3 [00:00<?, ?it/s]/home/bkhosra/.conda/envs/RP/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/bkhosra/.conda/envs/RP/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/bkhosra/.conda/envs/RP/lib/python3.11/site-packages/transformers/generation/utils.py:1637: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "Processing items: 100%|██████████| 3/3 [00:15<00:00,  5.27s/it]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "report_files = glob.glob(\"../../sample_reports/*.txt\")\n",
    "\n",
    "reports = []\n",
    "for file in report_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        reports.append({\"report\": f.read(), \"file_name\": file})\n",
    "\n",
    "engine(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d96b8b-2c21-4f58-944e-c18f3cbbf885",
   "metadata": {},
   "source": [
    "The engine will process each report and saves the results to `output_tutorial_6.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d40f7af-3083-40ac-8118-9f468601443a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_response_0</th>\n",
       "      <th>default_response_1</th>\n",
       "      <th>report</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n&lt;initial_answer&gt;\\nPresent\\n&lt;/initial_answer&gt;...</td>\n",
       "      <td>{\\n    \"answer\": \"Present\"\\n}</td>\n",
       "      <td>Clinical Information:\\n67-year-old male with s...</td>\n",
       "      <td>../../sample_reports/sample_report_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n&lt;initial_answer&gt;\\nPresent\\n&lt;/initial_answer&gt;...</td>\n",
       "      <td>{\\n    \"answer\": \"Present\"\\n}</td>\n",
       "      <td>Clinical Information:\\n72-year-old female with...</td>\n",
       "      <td>../../sample_reports/sample_report_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n&lt;initial_answer&gt;\\nAbsent\\n&lt;/initial_answer&gt;\\...</td>\n",
       "      <td>{\\n    \"answer\": \"Absent\"\\n}</td>\n",
       "      <td>Here is an example radiology report describing...</td>\n",
       "      <td>../../sample_reports/sample_report_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      default_response_0  \\\n",
       "index                                                      \n",
       "0      \\n<initial_answer>\\nPresent\\n</initial_answer>...   \n",
       "1      \\n<initial_answer>\\nPresent\\n</initial_answer>...   \n",
       "2      \\n<initial_answer>\\nAbsent\\n</initial_answer>\\...   \n",
       "\n",
       "                  default_response_1  \\\n",
       "index                                  \n",
       "0      {\\n    \"answer\": \"Present\"\\n}   \n",
       "1      {\\n    \"answer\": \"Present\"\\n}   \n",
       "2       {\\n    \"answer\": \"Absent\"\\n}   \n",
       "\n",
       "                                                  report  \\\n",
       "index                                                      \n",
       "0      Clinical Information:\\n67-year-old male with s...   \n",
       "1      Clinical Information:\\n72-year-old female with...   \n",
       "2      Here is an example radiology report describing...   \n",
       "\n",
       "                                      file_name  \n",
       "index                                            \n",
       "0      ../../sample_reports/sample_report_1.txt  \n",
       "1      ../../sample_reports/sample_report_2.txt  \n",
       "2      ../../sample_reports/sample_report_3.txt  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output_tutorial_6.csv\", index_col='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99904090",
   "metadata": {},
   "source": [
    "Finally, we save the log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab38e124-d51c-4e28-97ac-d8c7200402dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RadPrompter Version: 1.1.4\n",
      "Model: Phi3ForCausalLM\n",
      "Prompt TOML: /mnt/NAS3/homes/bkhosra/RadPrompter/tutorials/06_HuggingFace-Client/06_HuggingFace-Client.toml\n",
      "Prompt Version: 0.1\n",
      "Prompt Hash: 83cd2cee67b7e10589df0b8f9e9fd2f2\n",
      "Concurrency Factor: 1\n",
      "Start Time: 2024-05-20 12:18:22\n",
      "End Time: 2024-05-20 12:18:38\n",
      "Duration: 16.0\n",
      "Number of Items: 3\n",
      "Average Processing Time: 5.333333333333333\n",
      "\n",
      "\n",
      "-------------------- *** - Prompt Content - *** --------------------\n",
      "[METADATA]\n",
      "version = 0.1\n",
      "description = \"A sample prompt for RadPrompter\"\n",
      "\n",
      "[PROMPTS]\n",
      "system_prompt = \"You are a helpful assistant that has 20 years of experience in reading radiology reports and extracting data elements.\"\n",
      "\n",
      "user_prompt_intro = \"\"\"\n",
      "Carefully review the provided chest CT report (in the <report> tag). Ensure that each data element is accurately captured. Here is the report:\n",
      "<report>\n",
      "{{report}}\n",
      "</report>\n",
      "\"\"\"\n",
      "\n",
      "user_prompt_cot = \"\"\"\n",
      "I want you to extract the following data element from the report: 'Pulmonary Embolism'\n",
      "Here are your options and you can explicitly use one of these:\n",
      "  - `Present`\n",
      "  - `Absent`\n",
      "Hint: \"Indicate `Present` if the report explicitly mentions the patient has pulmonary embolism in their CT scan. Indicate `Absent` if pulmonary embolism is not seen or if a previously observed pulmonary embolism is mentioned as resolved.\n",
      "After you provide the data element, I will ask you to provide an explanation and then the final answer.\n",
      "Now give your initial answer. Then provide a step-by-step explanation based on the information in the report, using no more than three short sentences. You can use less sentences if needed.Try to critically appraise your initial answer, which MIGHT be wrong. Then give me your final answer.\n",
      "Format your answers with this format as:\n",
      "<answer>\n",
      "<initial_answer>\n",
      "initial answer goes here\n",
      "</initial_answer>\n",
      "<explanation>\n",
      "1. your first explanation goes here\n",
      "2. your second explanation goes here (if needed)\n",
      "3. your third explanation goes here (if needed) \n",
      "</explanation>\n",
      "<final_answer>\n",
      "final answer goes here\n",
      "</final_answer>\n",
      "</answer>\n",
      "\"\"\"\n",
      "\n",
      "user_prompt_format = \"\"\"\n",
      "Great. Now please format your response in JSON format like this:\n",
      "{\n",
      "    \"answer\": \"your final answer\"\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "[CONSTRUCTOR]\n",
      "system = \"rdp(system_prompt)\"\n",
      "user = [\"rdp(user_prompt_intro + user_prompt_cot)\", \"rdp(user_prompt_format)\"]\n",
      "response_templates = [\"<answer>\", \"\"]\n",
      "stop_tags = [\"</answer>\", \"}\"]\n"
     ]
    }
   ],
   "source": [
    "engine.save_log(\"log_tutorial_6.log\")\n",
    "\n",
    "with open(\"log_tutorial_6.log\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14990e0",
   "metadata": {},
   "source": [
    "The `HuggingFaceClient` can be useful for beginning to explore RadPrompter's capabilities, but it is certainly **not the best option**. We highly advise using the `vLLMClient` or `OllamaClient`, as they support concurrency and are more stable for batch document processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RadPrompter",
   "language": "python",
   "name": "rp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
