{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89716955-cfb6-4d70-bb82-0c6cc864e907",
   "metadata": {},
   "source": [
    "# Tutorial 6: HuggingFace Client\n",
    "\n",
    "In this tutorial, we'll explore how to use the `HuggingFaceClient`. However, be aware that this is not the recommended way of using RadPrompter and it is advised to use API based methods like `vLLMClient` or `OllamaClient` to have a much faster inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f9c2d",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "If you don't have `RadPrompter` installed, you can install it using pip:\n",
    "\n",
    "```bash\n",
    "pip install radprompter\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db7ff19-6d51-4b73-a0c2-c154090503aa",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "As always, we start by importing the `Prompt` class and creating a prompt object from a TOML file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be8a55b9-77be-4443-8568-37159027379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkhosra/.conda/envs/hf/lib/python3.10/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils. Support for replacing an already imported distutils is deprecated. In the future, this condition will fail. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='padding: 0; border-radius: 5px; font-family: Arial; line-height: 1.2rem; border: 1px solid currentColor'><div style='display: flex; align-items: top; padding: 0; border-right-width: 1px'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px; '>System:</h4><p style='margin: 0; padding: 8px; border-left: 1px solid currentColor;'>You are a helpful assistant that has 20 years of experience in reading radiology reports and extracting data elements.</p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>User:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'>Carefully review the provided chest CT report (in the &lt;report&gt; tag). Ensure that each data element is accurately captured. Here is the report:<br>&lt;report&gt;<br><span style='background-color: rgb(255, 224, 178, 0.3);'>{{report}}</span><br>&lt;/report&gt;<br>I want you to extract the following data element from the report: &#x27;Pulmonary Embolism&#x27;<br>Here are your options and you can explicitly use one of these:<br>  - `Present`<br>  - `Absent`<br>Hint: &quot;Indicate `Present` if the report explicitly mentions the patient has pulmonary embolism in their CT scan. Indicate `Absent` if pulmonary embolism is not seen or if a previously observed pulmonary embolism is mentioned as resolved.<br>After you provide the data element, I will ask you to provide an explanation and then the final answer.<br>Now give your initial answer. Then provide a step-by-step explanation based on the information in the report, using no more than three short sentences. You can use less sentences if needed.Try to critically appraise your initial answer, which MIGHT be wrong. Then give me your final answer.<br>Format your answers with this format as:<br>&lt;answer&gt;<br>&lt;initial_answer&gt;<br>initial answer goes here<br>&lt;/initial_answer&gt;<br>&lt;explanation&gt;<br>1. your first explanation goes here<br>2. your second explanation goes here (if needed)<br>3. your third explanation goes here (if needed) <br>&lt;/explanation&gt;<br>&lt;final_answer&gt;<br>final answer goes here<br>&lt;/final_answer&gt;<br>&lt;/answer&gt;<br></p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>Assistant:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'>&lt;answer&gt;<span style='background-color: rgb(178, 219, 255, 0.3);'>[... response ...]</span>&lt;/answer&gt;</p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>User:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'>Great. Now please format your response in JSON format like this:<br>{<br>    &quot;answer&quot;: &quot;your final answer&quot;<br>}<br></p></div><div style='display: flex; align-items: top; padding: 0;'><h4 style='margin: 0; padding: 8px; flex: 0 0 100px;'>Assistant:</h4><p style='margin: 0; padding: 8px; flex-grow: 1; border-left: 1px solid currentColor;border-top: 1px solid currentColor;'><span style='background-color: rgb(178, 219, 255, 0.3);'>[... response ...]</span>}</p></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from radprompter import Prompt\n",
    "\n",
    "prompt = Prompt('06_HuggingFace-Client.toml')\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e829b2c-79b3-45c4-8b40-1b3744fd6c29",
   "metadata": {},
   "source": [
    "## Client and Engine\n",
    "\n",
    "We'll use the new `HuggingFaceClient`. This model accepts the model and tokenizer from HuggingFace. This allows for all customizations on the model including quantization.\n",
    "\n",
    "Before running the next cell, make sure you have the following libraries installed:\n",
    "\n",
    "```bash\n",
    "pip install torch transformers flash_attn SentencePiece accelerate bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7730bec-691b-4727-98b8-f0ea4868a772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b67b36e4df34349ba1ae43bb65cb0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9ab47e53da4de6a0c1646307455fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca6f83229fb49a2825036c2cd8c1cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c81547c046b42d884958beeb552a7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f027d0676e2c48fa969b97f199fe9429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bbe1a72ad843d293f3e23279d6e00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_name = \"google/medgemma-4b-it\"\n",
    "\n",
    "quantization_conf = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"eager\", # This is required for Gemma models (https://github.com/google-deepmind/gemma/issues/169)\n",
    "    quantization_config=quantization_conf,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2be337f7-fbf2-40e6-b592-47c883402ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bkhosra/.conda/envs/hf/lib/python3.10/site-packages/radprompter/radprompter.py:26: UserWarning: Output file output_tutorial_6.csv already exists. The file will be **replaced** if you proceed with running the engine.\n",
      "  warnings.warn(f\"Output file {self.output_file} already exists. The file will be **replaced** if you proceed with running the engine.\")\n",
      "/home/bkhosra/.conda/envs/hf/lib/python3.10/site-packages/radprompter/radprompter.py:37: UserWarning: HuggingFace client does not support Pydantic models and will be set to False.\n",
      "  warnings.warn(\"HuggingFace client does not support Pydantic models and will be set to False.\")\n"
     ]
    }
   ],
   "source": [
    "from radprompter import RadPrompter, HuggingFaceClient\n",
    "\n",
    "client = HuggingFaceClient(\n",
    "    hf_model = model,\n",
    "    hf_tokenizer = tokenizer,\n",
    "    temperature = 0.0,\n",
    "    seed = 42\n",
    ")\n",
    "\n",
    "engine = RadPrompter(\n",
    "    client=client,\n",
    "    prompt=prompt, \n",
    "    output_file=\"output_tutorial_6.csv\",\n",
    "    concurrency=1,\n",
    "    hide_blocks=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc605fa7-99a5-47fa-a3b9-d4e3c23dbe54",
   "metadata": {},
   "source": [
    "And we run it on our sample reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be2ab6-e689-4dac-ab48-60c1a6d72167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "report_files = glob.glob(\"../../sample_reports/*.txt\")\n",
    "\n",
    "reports = []\n",
    "for file in report_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        reports.append({\"report\": f.read(), \"file_name\": file})\n",
    "\n",
    "engine(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d96b8b-2c21-4f58-944e-c18f3cbbf885",
   "metadata": {},
   "source": [
    "The engine will process each report and saves the results to `output_tutorial_6.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d40f7af-3083-40ac-8118-9f468601443a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_response</th>\n",
       "      <th>report</th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ERROR</td>\n",
       "      <td>Clinical Information:\\n67-year-old male with s...</td>\n",
       "      <td>../../sample_reports/sample_report_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ERROR</td>\n",
       "      <td>Clinical Information:\\n72-year-old female with...</td>\n",
       "      <td>../../sample_reports/sample_report_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ERROR</td>\n",
       "      <td>Here is an example radiology report describing...</td>\n",
       "      <td>../../sample_reports/sample_report_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      default_response                                             report  \\\n",
       "index                                                                       \n",
       "0                ERROR  Clinical Information:\\n67-year-old male with s...   \n",
       "1                ERROR  Clinical Information:\\n72-year-old female with...   \n",
       "2                ERROR  Here is an example radiology report describing...   \n",
       "\n",
       "                                      file_name  \n",
       "index                                            \n",
       "0      ../../sample_reports/sample_report_1.txt  \n",
       "1      ../../sample_reports/sample_report_2.txt  \n",
       "2      ../../sample_reports/sample_report_3.txt  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"output_tutorial_6.csv\", index_col='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99904090",
   "metadata": {},
   "source": [
    "Finally, we save the log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab38e124-d51c-4e28-97ac-d8c7200402dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RadPrompter Version: 2.0.0\n",
      "Model: Gemma3ForConditionalGeneration\n",
      "Seed: 42\n",
      "Temperature: 0.0\n",
      "Frequency Penalty: 0.0\n",
      "Top-P: 0.9\n",
      "Prompt TOML: /opt/localdata/Data/Datathon_BKh/LLMSynth/RadPrompter/tutorials/06_HuggingFace-Client/06_HuggingFace-Client.toml\n",
      "Prompt Version: 0.1\n",
      "Prompt Hash: 83cd2cee67b7e10589df0b8f9e9fd2f2\n",
      "Concurrency Factor: 1\n",
      "Use Pydantic: False\n",
      "Start Time: 2025-06-30 17:17:05\n",
      "End Time: 2025-06-30 17:17:05\n",
      "Duration: 0.0\n",
      "Number of Items: 3\n",
      "Average Processing Time: 0.0\n",
      "\n",
      "\n",
      "-------------------- *** - Prompt Content - *** --------------------\n",
      "[METADATA]\n",
      "version = 0.1\n",
      "description = \"A sample prompt for RadPrompter\"\n",
      "\n",
      "[PROMPTS]\n",
      "system_prompt = \"You are a helpful assistant that has 20 years of experience in reading radiology reports and extracting data elements.\"\n",
      "\n",
      "user_prompt_intro = \"\"\"\n",
      "Carefully review the provided chest CT report (in the <report> tag). Ensure that each data element is accurately captured. Here is the report:\n",
      "<report>\n",
      "{{report}}\n",
      "</report>\n",
      "\"\"\"\n",
      "\n",
      "user_prompt_cot = \"\"\"\n",
      "I want you to extract the following data element from the report: 'Pulmonary Embolism'\n",
      "Here are your options and you can explicitly use one of these:\n",
      "  - `Present`\n",
      "  - `Absent`\n",
      "Hint: \"Indicate `Present` if the report explicitly mentions the patient has pulmonary embolism in their CT scan. Indicate `Absent` if pulmonary embolism is not seen or if a previously observed pulmonary embolism is mentioned as resolved.\n",
      "After you provide the data element, I will ask you to provide an explanation and then the final answer.\n",
      "Now give your initial answer. Then provide a step-by-step explanation based on the information in the report, using no more than three short sentences. You can use less sentences if needed.Try to critically appraise your initial answer, which MIGHT be wrong. Then give me your final answer.\n",
      "Format your answers with this format as:\n",
      "<answer>\n",
      "<initial_answer>\n",
      "initial answer goes here\n",
      "</initial_answer>\n",
      "<explanation>\n",
      "1. your first explanation goes here\n",
      "2. your second explanation goes here (if needed)\n",
      "3. your third explanation goes here (if needed) \n",
      "</explanation>\n",
      "<final_answer>\n",
      "final answer goes here\n",
      "</final_answer>\n",
      "</answer>\n",
      "\"\"\"\n",
      "\n",
      "user_prompt_format = \"\"\"\n",
      "Great. Now please format your response in JSON format like this:\n",
      "{\n",
      "    \"answer\": \"your final answer\"\n",
      "}\n",
      "\"\"\"\n",
      "\n",
      "[CONSTRUCTOR]\n",
      "system = \"rdp(system_prompt)\"\n",
      "user = [\"rdp(user_prompt_intro + user_prompt_cot)\", \"rdp(user_prompt_format)\"]\n",
      "response_templates = [\"<answer>\", \"\"]\n",
      "stop_tags = [\"</answer>\", \"}\"]\n"
     ]
    }
   ],
   "source": [
    "engine.save_log(\"log_tutorial_6.log\")\n",
    "\n",
    "with open(\"log_tutorial_6.log\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14990e0",
   "metadata": {},
   "source": [
    "The `HuggingFaceClient` can be useful for beginning to explore RadPrompter's capabilities, but it is certainly **not the best option**. We highly advise using the `vLLMClient` or `OllamaClient`, as they support concurrency and are more stable for batch document processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HF",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
